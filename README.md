# generative-adversarial-neural-network

A simple implementation of a Generative Adversarial Neural Network (GANs) for generating numbers using the MINST dataset.

> Under development

The GANs are well-known neural network architecture for image generation. The basic idea behind GANS is to set 2 Artificial Neural Networks (ANN) to fight each other. The first one is the Discriminator, which has as its main goal to distinguish whether a certain image came from our dataset or is generated by the generator. The second model, the generator, as the name suggests, is responsible for generating the desired image based on a noisy image or the previous iteration image. Therefore, the role of the generator is to fool the discriminator by providing a very similar image. 

In this neural network architecture, the loss function plays an important role in guiding both models. The discriminator loss function will try to make its output to be nearly 1 for real image input and 0 for fake image input. On the other hand, as the generator has to deceive the discriminator, it will try to make the output more realistic, causing the minimization of part of the discriminator loss function that should be maximized by the discriminator. The details of both loss functions are depicted in the image below:

![gans](https://github.com/HerbertHipolito/generative-adversarial-neural-network/assets/94997683/b28e4147-7dc2-479c-b6b4-100867aeb37d)

In order to lead the generator model to create an image of a desired class, both the discriminator and generator receive an array (1,794) in which the last 10 elements represent the class and the rest is the image itself. Thereby, the user can choose a particular number in the image to be generated.

As my first implementation, I used an MLP architecture for the generator and discriminator models in a handwritten digit-based dataset (MINST). This dataset contains around 70 thousand of 28x28 grey images ranging from 0 to 9. The images are vectorized (1,784) to fit into the discriminator. The generator receives the concatenation between vectorized noisy image generated by a normal distribution and a (1,10) array that represents the class of the image which will be created. All elements of this array are the current class.

In regard to the discriminator and generator architecture, the generator is composed of 3 hidden layers with a Leaky ReLU activation function and 256, 512, and 1024 neurons in order. The output layer contains 784 neurons (vectorized image size) with a Hyperbolic tangent activation function. The discriminator has a similar structure with the reverse order of the number of hidden layer neurons and the Sigmoid activation function in the output layer.

![modelsGAns](https://github.com/HerbertHipolito/generative-adversarial-neural-network/assets/94997683/283fe337-6330-4f20-8861-e93b7af22a4e)

The results after using around 40 epochs, discriminator rate learning = 1e-5, generator rate learning = 4e-5, and 3 numbers set up (1, 4, 7) is displayed  below:

![collage](https://github.com/HerbertHipolito/generative-adversarial-neural-network/assets/94997683/28aad9c2-590d-4c61-9dd8-6f1b382555f9)

I just selected 3 numbers because it can take a considerable time to training a GAN model.
The GIF below shows the process of creation of a image.

![gans](https://github.com/HerbertHipolito/generative-adversarial-neural-network/assets/94997683/52eac4bf-df49-484b-977a-6f1994a1ac22)

The GANs are really interesting ANN architecture. However, its training may be hard as GANs are unstable, leading easily to gradient vanishing. 