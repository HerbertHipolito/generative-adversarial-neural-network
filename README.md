# generative-adversarial-neural-network

A simple implementation of a Generative Adversarial Neural Network (GANs) for generating numbers using the MINST dataset.

> Under development

The GANs are well-known neural network architecture for image generation. The basic idea behind GANS is to set 2 Artificial Neural Networks (ANN) to fight each other. The first one is the Discriminator, which has as its main goal to distinguish whether a certain image came from our dataset or is generated by the Generator. The second model, the generator, as the name suggests, is responsible for generating the desired image based on a noisy image or the previous iteration image. Therefore, the role of the generator is to fool the discriminator by providing a very similar image. 

In this neural network architecture, the loss function plays an important role in guiding both models. The discriminator loss function will try to make the its output to be nearly 1 for real image input and 0 for fake image input. On the other hand, as the generator has to deceive the discriminator, the generator model will minimize part of the discriminator loss function. it will make the generator output more realistic images. The details of both loss functions are described in the image below:

![gans](https://github.com/HerbertHipolito/generative-adversarial-neural-network/assets/94997683/b28e4147-7dc2-479c-b6b4-100867aeb37d)

As my first implementation, I used an MLP architecture for the generator and discriminator models in a handwritten digit-based dataset (MINST). This dataset contains around 70 thousand of 28x28 grey images ranging from 0 to 9. The images are vectorized to fit into the discriminator (1,784). The generator receives the concatenation between vectorized noisy image generated by a normal distribution and a (1,10) array that represents the class of the image which will be created. All elements of this array are the current class.

In regard to the discriminator and generator architecture, the generator is composed of 3 hidden layers with a Leaky ReLU activation function and 256, 512, and 1024 neurons in order. The output layer contains 784 neurons (vectorized image size) with a Hyperbolic tangent activation function. The discriminator has a similar structure with the reverse order of the number of hidden layer neurons and the Sigmoid activation function in the output layer.

The GANs are really interesting ANN architecture. However, its training may be hard as GANs are unstable, leading easily to gradient vanishing. 

![image](https://github.com/HerbertHipolito/generative-adversarial-neural-network/assets/94997683/6f3e0064-69d8-480d-8028-21717a9dc0e1)
